[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "github-streak\n\n\n\npython\n\ngithub\n\nghapi\n\n\n\n\n\n\n\n\n\nNov 1, 2024\n\n\nDeven Mistry\n\n\n\n\n\n\n\n\n\n\n\n\ntalk-to-a-file\n\n\n\npython\n\nanthropic\n\nllms\n\npapers\n\n\n\n\n\n\n\n\n\nMay 31, 2024\n\n\nDeven Mistry\n\n\n\n\n\n\n\n\n\n\n\n\nndbev-oauth\n\n\n\npython\n\ntodo\n\n\n\n\n\n\n\n\n\nNov 18, 2022\n\n\nDeven Mistry\n\n\n\n\n\n\n\n\n\n\n\n\nMasters thesis\n\n\n\npython\n\ntodo\n\n\n\n\n\n\n\n\n\nNov 18, 2022\n\n\nDeven Mistry\n\n\n\n\n\n\n\n\n\n\n\n\nyt-video-annotator\n\n\n\npython\n\ntodo\n\n\n\n\n\n\n\n\n\nNov 18, 2022\n\n\nDeven Mistry\n\n\n\n\n\n\n\n\n\n\n\n\nwrites-like-you\n\n\n\npython\n\ntodo\n\n\n\n\n\n\n\n\n\nNov 18, 2022\n\n\nDeven Mistry\n\n\n\n\n\n\n\n\n\n\n\n\nclean-plot\n\n\n\npython\n\ntodo\n\n\n\n\n\n\n\n\n\nNov 18, 2022\n\n\nDeven Mistry\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/articles/mistry2023comparative.html",
    "href": "publications/articles/mistry2023comparative.html",
    "title": "A Comparative Study of Sentence Embedding Models for Assessing Semantic Variation",
    "section": "",
    "text": "Mistry, Deven Mahesh and Minai, Ali A (2023). A Comparative Study of Sentence Embedding Models for Assessing Semantic Variation. International Conference on Artificial Neural Networks. pg 1-12."
  },
  {
    "objectID": "publications/articles/mistry2023comparative.html#citation-apa-7",
    "href": "publications/articles/mistry2023comparative.html#citation-apa-7",
    "title": "A Comparative Study of Sentence Embedding Models for Assessing Semantic Variation",
    "section": "",
    "text": "Mistry, Deven Mahesh and Minai, Ali A (2023). A Comparative Study of Sentence Embedding Models for Assessing Semantic Variation. International Conference on Artificial Neural Networks. pg 1-12."
  },
  {
    "objectID": "publications/articles/mistry2023comparative.html#abstract",
    "href": "publications/articles/mistry2023comparative.html#abstract",
    "title": "A Comparative Study of Sentence Embedding Models for Assessing Semantic Variation",
    "section": "Abstract",
    "text": "Abstract\nAnalyzing the pattern of semantic variation in long real-world texts such as books or transcripts is interesting from the stylistic, cognitive, and linguistic perspectives. It is also useful for applications such as text segmentation, document summarization, and detection of semantic novelty. The recent emergence of several vector-space methods for sentence embedding has made such analysis feasible. However, this raises the issue of how consistent and meaningful the semantic representations produced by various methods are in themselves. In this paper, we compare several recent sentence embedding methods via time-series of semantic similarity between successive sentences and matrices of pairwise sentence similarity for multiple books of literature. In contrast to previous work using target tasks and curated datasets to compare sentence embedding methods, our approach provides an evaluation of the methods ‘in the wild’. We find that most of the sentence embedding methods considered do infer highly correlated patterns of semantic similarity in a given document, but show interesting differences."
  },
  {
    "objectID": "TIL/nohup.html",
    "href": "TIL/nohup.html",
    "title": "nohup and &",
    "section": "",
    "text": "Yesterday, I ran into a problem where I wanted to run a shell script in the background on a linux machine. I knew that\nwould solve the problem for me, however I was working on a remote machine, which meant that closing the ssh connection would terminate the execution of the command.\nEnter nohup. This is a wonderful bash utility which allows you to run your script in the background on a remote machine even when you close the connection.\nnohup is just short for no hang up. You can use this command in multiple ways. There’s a wonderful article on DigitalOcean1 explaining this command. I’ll just list a few of them here.\nBy default, nohup will save the output of the execution in nohup.out. You can change that with the redirect output command.\nHere’s another good reference from StackOverFlow2"
  },
  {
    "objectID": "TIL/nohup.html#footnotes",
    "href": "TIL/nohup.html#footnotes",
    "title": "nohup and &",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDigitalOcean↩︎\nStackOverFlow↩︎"
  },
  {
    "objectID": "TIL/viewing-images-in-terminal.html",
    "href": "TIL/viewing-images-in-terminal.html",
    "title": "Viewing images in the terminal",
    "section": "",
    "text": "The ability to view images in a terminal is a very handy skill. I recently ran into this issue when I was working on a project which had over a million images on a network drive.\nThe problem became a complex one to solve as even running a mere ls command was not an option on the network drive.\nFiring up a jupyter notebook server everytime to view an image was not a feasible option. And there had to be a better way.\nUpon some searching I found some popular options\n\nfeh\nTerminalImageViewer (tiv)\nViu\n\nIn my use case, the best one which worked the best in my use case was viu because of its support for jpgs."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "About this blog\nHello everyone, I am Deven. I am currently a PhD student at Indiana University Bloomington. I am a part of Dr. Zoran Tiganj’s Lab.\nBefore this, I was a Full Stack ML Developer at GQC, where I worked on a lot of interesting projects ranging from flood detection, flood segmentation, LLM finetuning and much more.\nDuring my Masters, I was at the University of Cincinnati, where I was a part of Dr. Ali Minai’s Complex Adaptive Systems Laboratory. My masters thesis can be found here"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Autograder\n\n\n\nteaching\n\ngithub-actions\n\nautograder\n\n\n\nAutograder\n\n\n\n\n\nOct 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nExperience with Distributed Training\n\n\n\npytorch\n\nddp\n\n\n\n\n\n\n\n\n\nApr 10, 2024\n\n\nDeven Mistry\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\nnews\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nNov 14, 2022\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nNov 11, 2022\n\n\nTristan O’Malley\n\n\n\n\n\n\n\n\n\n\n\n\nCleaning text for NLP tasks\n\n\n\ntext preprocessing\n\ntokenization\n\nlemmatization\n\nregex\n\nnltk\n\n\n\nA simple guide for cleaning text\n\n\n\n\n\nJun 15, 2021\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deven’s blog",
    "section": "",
    "text": "Autograder\n\n\n\nteaching\n\ngithub-actions\n\nautograder\n\n\n\nAutograder\n\n\n\n\n\nOct 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nExperience with Distributed Training\n\n\n\npytorch\n\nddp\n\n\n\n\n\n\n\n\n\nApr 10, 2024\n\n\nDeven Mistry\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\nnews\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nNov 14, 2022\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nNov 11, 2022\n\n\nTristan O’Malley\n\n\n\n\n\n\n\n\n\n\n\n\nCleaning text for NLP tasks\n\n\n\ntext preprocessing\n\ntokenization\n\nlemmatization\n\nregex\n\nnltk\n\n\n\nA simple guide for cleaning text\n\n\n\n\n\nJun 15, 2021\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/github-streak.html",
    "href": "projects/github-streak.html",
    "title": "github-streak",
    "section": "",
    "text": "github-streak"
  },
  {
    "objectID": "projects/yt-video-annotator.html",
    "href": "projects/yt-video-annotator.html",
    "title": "yt-video-annotator",
    "section": "",
    "text": "yt-video-annotator"
  },
  {
    "objectID": "projects/talk-to-a-paper.html",
    "href": "projects/talk-to-a-paper.html",
    "title": "talk-to-a-file",
    "section": "",
    "text": "talk-to-a-file"
  },
  {
    "objectID": "projects/ndbev-oauth.html",
    "href": "projects/ndbev-oauth.html",
    "title": "ndbev-oauth",
    "section": "",
    "text": "ndbev-oauth"
  },
  {
    "objectID": "projects/masters-thesis.html",
    "href": "projects/masters-thesis.html",
    "title": "Masters thesis",
    "section": "",
    "text": "Masters thesis"
  },
  {
    "objectID": "projects/writes-like-you.html",
    "href": "projects/writes-like-you.html",
    "title": "writes-like-you",
    "section": "",
    "text": "writes-like-you"
  },
  {
    "objectID": "projects/clean-plot.html",
    "href": "projects/clean-plot.html",
    "title": "clean-plot",
    "section": "",
    "text": "clean-plot"
  },
  {
    "objectID": "posts/2021-06-15-Cleaning text for NLP tasks.html",
    "href": "posts/2021-06-15-Cleaning text for NLP tasks.html",
    "title": "Cleaning text for NLP tasks",
    "section": "",
    "text": "I started working in the field of Natual Language Processing back in August 2020. I am no expert in this field but in the past few months that I have spent my time cleaning textual data from different sources, I did manage to learn a few things and I am here to share them. These tips/suggestions are coming from someone who has had no prior experience in NLP at all. I hope whoever is reading this gets to learn something out of it. With that being said, let’s get started!\n\n\n\nThere are a few simple parameters which people don’t often use while read txt files"
  },
  {
    "objectID": "posts/2021-06-15-Cleaning text for NLP tasks.html#some-background",
    "href": "posts/2021-06-15-Cleaning text for NLP tasks.html#some-background",
    "title": "Cleaning text for NLP tasks",
    "section": "",
    "text": "I started working in the field of Natual Language Processing back in August 2020. I am no expert in this field but in the past few months that I have spent my time cleaning textual data from different sources, I did manage to learn a few things and I am here to share them. These tips/suggestions are coming from someone who has had no prior experience in NLP at all. I hope whoever is reading this gets to learn something out of it. With that being said, let’s get started!"
  },
  {
    "objectID": "posts/2021-06-15-Cleaning text for NLP tasks.html#reading-txt-files",
    "href": "posts/2021-06-15-Cleaning text for NLP tasks.html#reading-txt-files",
    "title": "Cleaning text for NLP tasks",
    "section": "",
    "text": "There are a few simple parameters which people don’t often use while read txt files"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Deven Mistry",
    "section": "",
    "text": "Home\n    Publications\n  \n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Author\n      \n      \n        Publication\n      \n      \n        Year\n      \n    \n  \n    \n      \n      \n    \n\n\n\n  \n    Emergence of Episodic Memory in Transformers - Characterizing Changes in Temporal Structure of Attention Scores During Training\n    Mistry, Deven Mahesh; Bajaj, Anooshka; Aggarwal, Yash; Maini, Sahaj Singh; Tiganj, Zoran\n    arXiv Preprint\n    (2025)\n    \n      Details\n    \n    \n    \n      DOI\n    \n    \n    \n    \n       Preprint\n    \n    \n    \n  \n\n  \n    A Comparative Study of Sentence Embedding Models for Assessing Semantic Variation\n    Mistry, Deven Mahesh and Minai, Ali A\n    International Conference on Artificial Neural Networks\n    (2023)\n    \n      Details\n    \n    \n    \n      DOI\n    \n    \n    \n    \n       Preprint\n    \n    \n    \n  \n\n  \n    A Systematic Comparative Study of Sentence Embedding Methods Using Real-World Text Corpora\n    Mistry, Deven Mahesh\n    University of Cincinnati\n    (2021)\n    \n      Details\n    \n    \n    \n    \n    \n       Materials\n    \n    \n  \n\n\n\nNo matching items"
  },
  {
    "objectID": "til.html",
    "href": "til.html",
    "title": "Today I Learned (TIL)",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n      \n        Categories\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nCategories\n\n\n\n\n\n\n\n\nNov 1, 2024\n\n\nLocked SQLite DB\n\n\npython, sqlite\n\n\n\n\n\n\nOct 24, 2024\n\n\nViewing images in the terminal\n\n\nc, cpp, rust, terminal\n\n\n\n\n\n\nMar 20, 2024\n\n\nnohup and &\n\n\nlinux, bash\n\n\n\n\n\n\nNov 18, 2022\n\n\nisinstance vs type and == vs is\n\n\npython\n\n\n\n\n\n\nNov 15, 2022\n\n\nLearning from pkl files\n\n\npkl, pytorch, fastai\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "TIL/learning-from-pkl-files.html",
    "href": "TIL/learning-from-pkl-files.html",
    "title": "Learning from pkl files",
    "section": "",
    "text": "When we train a model using fastai and export it using learn.export, you need to re-declare the functions that you had used to train the model.\nLet me show you what I mean by that,\nfrom fastai.vision.all import *\nset_seed(99, True)\npath = untar_data(URLs.PETS)/'images'\n\ndef label_func(x): return x[0].isupper()\n\ndls = ImageDataLoaders.from_name_func(\n    path, get_image_files(path), valid_pct=0.2,\n    label_func=label_func, item_tfms=Resize(224))\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(1)\nlearn.export('saved-model.pkl')\nHere, we are training a simple model for image classification and then exporting the saved model.\nNow, if you were to use this model in a different jupyter notebook or a py file using the load_learner function.\nfrom fastai.vision.all import *\nlearn = load_learner('saved-model.pkl')\nYou would get an error something like this…\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-3-c010bc50794d&gt; in &lt;module&gt;\n----&gt; 1 learn = load_learner('saved-model.pkl')\n\n3 frames\n/usr/local/lib/python3.7/dist-packages/torch/serialization.py in find_class(self, mod_name, name)\n   1040                     pass\n   1041             mod_name = load_module_mapping.get(mod_name, mod_name)\n-&gt; 1042             return super().find_class(mod_name, name)\n   1043\n   1044     # Load the data (which may in turn use `persistent_load` to load tensors)\n\nAttributeError: Custom classes or functions exported with your `Learner` not available in namespace.\\Re-declare/import before loading:\n    Can't get attribute 'label_func' on &lt;module '__main__'&gt;\nThis error is essentially saying that, before you load your model, the script (py file) or the notebook is looking for the function label_func.\nPreviously, I would just copy that whole function again from the previous file and paste it in my new file, something like this.\nfrom fastai.vision.all import *\ndef label_func(x): return x[0].isupper()\n\nlearn = load_learner('saved-model.pkl')\nBut, as it turns out, the load_learner is just looking for a reference to the label_func and not it’s entire definition. So, in theory, you could have your code look something like…\nfrom fastai.vision.all import *\n\n## notice the empty function declaration\ndef label_func(x): pass\n\nlearn = load_learner('saved-model.pkl')\nWith this declaration, your inference would work just fine and you won’t get any problems.\n\n\n\n\n\n\nNote\n\n\n\nUse this method only, when you wish to do inference with your model.  If you wish to re-train the model, you would need a new dataloader, label_func and the whole nine-yards.\n\n\nI hope this was helpful. Happy Learning. Cheers!"
  },
  {
    "objectID": "TIL/isinstance-vs-type.html",
    "href": "TIL/isinstance-vs-type.html",
    "title": "isinstance vs type and == vs is",
    "section": "",
    "text": "There are multiple ways in Python to achieve a single task and at times you can get away with implementing things the wrong way and getting the correct answer.\nThis TIL has come into existence as I struggled with a issue that I was trying to fix at work."
  },
  {
    "objectID": "TIL/isinstance-vs-type.html#isinstance-vs-type",
    "href": "TIL/isinstance-vs-type.html#isinstance-vs-type",
    "title": "isinstance vs type and == vs is",
    "section": "isinstance vs type",
    "text": "isinstance vs type\nWe’ve all used the type and the isinstance method at some point in our functions.\ns = 'foo'\nif type(s) == str:\n    print('this variable is of string type')\n\nif isinstance(s, str):\n    print('this variable is of string type')\nIf you were to execute this code, you would get the same output.\nBut what’s the difference between them?\ninstance can check types of even derived classes (meaning, it is intended to be used in cases involving inheritence) whereas type is not.\nTLDR; 1\nclass Person: pass\n\nclass Student(Person): pass\n\n\nisinstance(Person(), Person)        # returns True\ntype(Student()) == Student          # returns True\nisinstance(Student(), Person)       # returns True\ntype(Student()) == Person           # returns False, and this probably won't be what you want.\nThere’s a similar confusion between is and ==."
  },
  {
    "objectID": "TIL/isinstance-vs-type.html#is-vs",
    "href": "TIL/isinstance-vs-type.html#is-vs",
    "title": "isinstance vs type and == vs is",
    "section": "is vs ==",
    "text": "is vs ==\nis looks for the same object (in memory), whereas == looks for the values referred by the variables.\nTLDR; 2\nn = 5\nif n == 5: print('Yep!')    # prints Yep!\nif n is 5: print('Yay!')    # prints Yep!\n\nL = []\nL.append(1)\n\nif L == [1]: print('Yay!')  # prints Yep!\nif L is [1]: print('Yay!')  # prints nothing\nH/T to Zach Mueller for helping me understand this"
  },
  {
    "objectID": "TIL/isinstance-vs-type.html#footnotes",
    "href": "TIL/isinstance-vs-type.html#footnotes",
    "title": "isinstance vs type and == vs is",
    "section": "Footnotes",
    "text": "Footnotes\n\n\ndifferences between type() and isinstance()?↩︎\ndifference between “==” and “is”?↩︎"
  },
  {
    "objectID": "publications/articles/mistry2021systematic.html",
    "href": "publications/articles/mistry2021systematic.html",
    "title": "A Systematic Comparative Study of Sentence Embedding Methods Using Real-World Text Corpora",
    "section": "",
    "text": "Mistry, Deven Mahesh (2021). A Systematic Comparative Study of Sentence Embedding Methods Using Real-World Text Corpora. University of Cincinnati."
  },
  {
    "objectID": "publications/articles/mistry2021systematic.html#citation-apa-7",
    "href": "publications/articles/mistry2021systematic.html#citation-apa-7",
    "title": "A Systematic Comparative Study of Sentence Embedding Methods Using Real-World Text Corpora",
    "section": "",
    "text": "Mistry, Deven Mahesh (2021). A Systematic Comparative Study of Sentence Embedding Methods Using Real-World Text Corpora. University of Cincinnati."
  },
  {
    "objectID": "publications/articles/mistry2021systematic.html#abstract",
    "href": "publications/articles/mistry2021systematic.html#abstract",
    "title": "A Systematic Comparative Study of Sentence Embedding Methods Using Real-World Text Corpora",
    "section": "Abstract",
    "text": "Abstract\nMany natural language processing (NLP) tasks require the conversion of textual data to numeric representations. Vector-space representations are the most popular way to do this. Initial vector-space models were used to represent individual words, but several very complex language models have been developed recently that can generate vector-space representations of sentences, paragraphs, and even entire documents. These models use various deep learning architectures including simple RNNs, stacked LSTMs, and Transformers [54]. Typically, the models are evaluated on synthetic or carefully curated benchmark datasets such as GLUE [56], SQuAD [45], COCO [55], etc. and tasks such as sentiment analysis and text classification. However, it is often unclear whether performance on these controlled benchmarks can transfer to non-curated, real-world datasets with uncontrolled semantic noise and complex structure. The goals of this thesis are: 1) To develop a methodology for systematically comparing a representative set of sentence encoder models on real-world texts; and 2) To apply this methodology using several sizeable real-world texts to arrive at a definitive ranking of the methods. The methodology uses the pattern of semantic similarity between sentence pairs to obtain a representation of semantic structure for each document using each encoding method. These structures are then compared statistically, through visualization, and through manual scoring to assess the relative quality of the representations produced by each encoding method. An innovative aspect of this research is the use of multiple English language translations of the same text as a further cross-validation mechanism."
  },
  {
    "objectID": "publications/articles/mistry2025emergence.html",
    "href": "publications/articles/mistry2025emergence.html",
    "title": "Emergence of Episodic Memory in Transformers - Characterizing Changes in Temporal Structure of Attention Scores During Training",
    "section": "",
    "text": "Mistry, D. M., Bajaj, A., Aggarwal, Y., Maini, S. S., & Tiganj, Z. (2025). Emergence of Episodic Memory in Transformers: Characterizing Changes in Temporal Structure of Attention Scores During Training. arXiv preprint arXiv:2502.06902."
  },
  {
    "objectID": "publications/articles/mistry2025emergence.html#citation-apa-7",
    "href": "publications/articles/mistry2025emergence.html#citation-apa-7",
    "title": "Emergence of Episodic Memory in Transformers - Characterizing Changes in Temporal Structure of Attention Scores During Training",
    "section": "",
    "text": "Mistry, D. M., Bajaj, A., Aggarwal, Y., Maini, S. S., & Tiganj, Z. (2025). Emergence of Episodic Memory in Transformers: Characterizing Changes in Temporal Structure of Attention Scores During Training. arXiv preprint arXiv:2502.06902."
  },
  {
    "objectID": "publications/articles/mistry2025emergence.html#abstract",
    "href": "publications/articles/mistry2025emergence.html#abstract",
    "title": "Emergence of Episodic Memory in Transformers - Characterizing Changes in Temporal Structure of Attention Scores During Training",
    "section": "Abstract",
    "text": "Abstract\nWe investigate in-context temporal biases in attention heads and transformer outputs. Using cognitive science methodologies, we analyze attention scores and outputs of the GPT-2 models of varying sizes. Across attention heads, we observe effects characteristic of human episodic memory, including temporal contiguity, primacy and recency. Transformer outputs demonstrate a tendency toward in-context serial recall. Importantly, this effect is eliminated after the ablation of the induction heads, which are the driving force behind the contiguity effect. Our findings offer insights into how transformers organize information temporally during in-context learning, shedding light on their similarities and differences with human memory and learning."
  }
]
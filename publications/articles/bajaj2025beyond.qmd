---
title: Beyond Semantics - How Temporal Biases Shape Retrieval in Transformer and State-Space Models
type: "article"
publication: "arXiv Preprint"
year: 2025
author: Bajaj, Anooshka; Mistry, Deven Mahesh; Maini, Sahaj Singh; Aggarwal, Yash; Tiganj, Zoran
preprint: "https://arxiv.org/abs/2510.22752"
doi: "https://doi.org/10.48550/arXiv.2510.22752"
toc: false
categories:
  - language models
  - episodic memory
  - free recall
---
## Citation (APA 7)

> Bajaj, A., Mistry, D. M., Maini, S. S., Aggarwal, Y., & Tiganj, Z. (2025). Beyond Semantics: How Temporal Biases Shape Retrieval in Transformer and State-Space Models. arXiv preprint arXiv:2510.22752

## Abstract

In-context learning is governed by both temporal and semantic relationships, shaping how Large Language Models (LLMs) retrieve contextual information.
Analogous to human episodic memory, where the retrieval of specific events is enabled by separating events that happened at different times, this work probes
the ability of various pretrained LLMs, including transformer and state-space models, to differentiate and retrieve temporally separated events. Specifically,
we prompted models with sequences containing multiple presentations of the same token, which reappears at the sequence end. By fixing the positions of these
repeated tokens and permuting all others, we removed semantic confounds and isolated temporal effects on next-token prediction. Across diverse sequences,
models consistently placed the highest probabilities on tokens following a repeated token, but with a notable bias for those nearest the beginning or end of
the input. An ablation experiment linked this phenomenon in transformers to induction heads. Extending the analysis to unique semantic contexts with partial
overlap further demonstrated that memories embedded in the middle of a prompt are retrieved less reliably. Despite architectural differences, state-space and
transformer models showed comparable temporal biases. Our findings deepen the understanding of temporal biases in in-context learning and offer an
illustration of how these biases can enable temporal separation and episodic retrieval.


